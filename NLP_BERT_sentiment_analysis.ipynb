{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c23ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktrain\n",
      "  Downloading ktrain-0.38.0.tar.gz (25.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.3 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (1.3.4)\n",
      "Collecting fastprogress>=0.1.21\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (2.27.1)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (1.1.0)\n",
      "Requirement already satisfied: packaging in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from ktrain) (21.3)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting syntok>1.3.3\n",
      "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
      "Collecting tika\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "Collecting transformers>=4.17.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp37-cp37m-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras_bert>=0.86.0\n",
      "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
      "Collecting whoosh\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from keras_bert>=0.86.0->ktrain) (1.21.2)\n",
      "Collecting keras-transformer==0.40.0\n",
      "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
      "Collecting keras-pos-embd==0.13.0\n",
      "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
      "Collecting keras-multi-head==0.29.0\n",
      "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
      "Collecting keras-layer-normalization==0.16.0\n",
      "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
      "Collecting keras-position-wise-feed-forward==0.8.0\n",
      "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
      "Collecting keras-embed-sim==0.10.0\n",
      "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
      "Collecting keras-self-attention==0.51.0\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->ktrain) (1.16.0)\n",
      "Collecting regex>2016\n",
      "  Downloading regex-2023.10.3-cp37-cp37m-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp37-cp37m-macosx_10_7_x86_64.whl (440 kB)\n",
      "\u001b[K     |████████████████████████████████| 440 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from transformers>=4.17.0->ktrain) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from transformers>=4.17.0->ktrain) (4.8.2)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[K     |████████████████████████████████| 268 kB 24.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.17.0->ktrain) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.17.0->ktrain) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from requests->ktrain) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from requests->ktrain) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from requests->ktrain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from requests->ktrain) (2022.12.7)\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from scikit-learn->ktrain) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from scikit-learn->ktrain) (2.2.0)\n",
      "Requirement already satisfied: setuptools in ./opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages (from tika->ktrain) (58.0.4)\n",
      "Building wheels for collected packages: ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, jieba, langdetect, tika\n",
      "  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.38.0-py3-none-any.whl size=25319980 sha256=52164f5677abf7f861ea85b374844806c9bdecc712bd87c4e9686dd3d117343b\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/cd/12/72/68d4a56716592293ce7afe21912cf5b54cf12e939bf0872c01\n",
      "  Building wheel for keras-bert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=0d05f8c1c7281ac53e4cce10a35c358fbcdda3dd696a72ad1ce5e56a88f53411\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=a6bd42bff951e5b61103cb4d64ee7daf754d95dda27c8385018f70d3bddad96a\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=eac2786e015a4e5ad2ac45aaf6eb7f22f2a7502aa655432ecaeaf3ac27f94e13\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=8f08f3653a88f719eff1acfbe96ce159e4d64261650db033ef4e921125a28a77\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=a67ab550759152361265831802dcb125ae59c568f24e6435c14203c62227b472\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=6d444a61383cb204a37a44190a30262c43636a3123c8ccf9579dff0605e0c861\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=2266e01eb15b5f16595de0903075cb7cb5cec1332665704eaeb77fff67e343ff\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=dc8413cb5e923102a8a8a28a8eaf0951c0ab398a93843b22b51a0400b35a7ef3\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314478 sha256=8c16405a279e0bc17536867689701d92fb0d4d276a052159540e7e0dd23885a4\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/24/aa/17/5bc7c72e9a37990a9620cc3aad0acad1564dcff6dbc2359de3\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=5b8b1649856b9f03100f774854745cd8d8024a9013447f5e4d1db284b6635b26\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32642 sha256=d310ccbfc2528986a5a51b503e2966626529789d5d8d050171f2ffb322891b2b\n",
      "  Stored in directory: /Users/mzafsar/Library/Caches/pip/wheels/d0/e8/f2/4d6ee3cf46b79e22dcc7d4cdcbeed804c985d346b44a213672\n",
      "Successfully built ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention jieba langdetect tika\n",
      "Installing collected packages: keras-self-attention, tqdm, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, fsspec, filelock, tokenizers, safetensors, regex, keras-transformer, huggingface-hub, whoosh, transformers, tika, syntok, sentencepiece, langdetect, keras-bert, jieba, fastprogress, chardet, cchardet, ktrain\n",
      "Successfully installed cchardet-2.1.7 chardet-5.2.0 fastprogress-1.0.3 filelock-3.12.2 fsspec-2023.1.0 huggingface-hub-0.16.4 jieba-0.42.1 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.38.0 langdetect-1.0.9 regex-2023.10.3 safetensors-0.4.0 sentencepiece-0.1.99 syntok-1.4.4 tika-2.6.0 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.30.2 whoosh-2.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e1bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzafsar/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ee44e",
   "metadata": {},
   "source": [
    "# part 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bffe088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 19s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\",\n",
    "                                  origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "                                  extract=True)\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7469fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mzafsar/.keras/datasets\n",
      "/Users/mzafsar/.keras/datasets/aclImdb\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(dataset))\n",
    "print(IMDB_DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3be85",
   "metadata": {},
   "source": [
    "# Creating the training test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6aa7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test), preproc = text.texts_from_folder(datadir = IMDB_DATADIR,\n",
    "                                                                       classes = ['pos', 'neg'],\n",
    "                                                                       maxlen = 500, \n",
    "                                                                       train_test_names = ['train', 'test'],\n",
    "                                                                       preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a957bf",
   "metadata": {},
   "source": [
    "# Building the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5502134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier(name = 'bert',\n",
    "                            train_data = (X_train, y_train),\n",
    "                            preproc = preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c1e93",
   "metadata": {},
   "source": [
    "# Training the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f03485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the learner instance\n",
    "learner = ktrain.get_learner(model = model,\n",
    "                             train_data = (X_train, y_train),\n",
    "                             val_data = (X_test, y_test),\n",
    "                             batch_size = 6 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      " 1254/25000 [>.............................] - ETA: 192:31:15 - loss: 0.4904 - accuracy: 0.7679"
     ]
    }
   ],
   "source": [
    "#Execute the final training\n",
    "learner.fit_onecycle(lr = 2e-5, \n",
    "                    epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7e43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
